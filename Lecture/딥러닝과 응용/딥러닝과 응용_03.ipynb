{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Classification  p6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost: 0.6240718\n",
      "200 cost: 0.5959397\n",
      "400 cost: 0.57166815\n",
      "600 cost: 0.5496109\n",
      "800 cost: 0.5290908\n",
      "1000 cost: 0.50976187\n",
      "1200 cost: 0.491438\n",
      "1400 cost: 0.47401258\n",
      "1600 cost: 0.45741963\n",
      "1800 cost: 0.441613\n",
      "2000 cost: 0.42655668\n",
      "2200 cost: 0.4122189\n",
      "2400 cost: 0.39856985\n",
      "2600 cost: 0.38558033\n",
      "2800 cost: 0.3732213\n",
      "3000 cost: 0.36146381\n",
      "3200 cost: 0.35027918\n",
      "3400 cost: 0.3396388\n",
      "3600 cost: 0.32951507\n",
      "3800 cost: 0.3198809\n",
      "4000 cost: 0.3107099\n",
      "4200 cost: 0.30197695\n",
      "4400 cost: 0.29365775\n",
      "4600 cost: 0.2857292\n",
      "4800 cost: 0.27816936\n",
      "5000 cost: 0.27095738\n",
      "5200 cost: 0.26407355\n",
      "5400 cost: 0.25749925\n",
      "5600 cost: 0.25121698\n",
      "5800 cost: 0.24521005\n",
      "6000 cost: 0.2394631\n",
      "6200 cost: 0.23396142\n",
      "6400 cost: 0.22869134\n",
      "6600 cost: 0.2236402\n",
      "6800 cost: 0.21879566\n",
      "7000 cost: 0.21414666\n",
      "7200 cost: 0.20968252\n",
      "7400 cost: 0.20539315\n",
      "7600 cost: 0.20126952\n",
      "7800 cost: 0.1973029\n",
      "8000 cost: 0.19348496\n",
      "8200 cost: 0.18980809\n",
      "8400 cost: 0.18626505\n",
      "8600 cost: 0.18284917\n",
      "8800 cost: 0.17955403\n",
      "9000 cost: 0.17637366\n",
      "9200 cost: 0.17330249\n",
      "9400 cost: 0.17033525\n",
      "9600 cost: 0.16746698\n",
      "9800 cost: 0.16469301\n",
      "10000 cost: 0.16200896\n",
      "\n",
      " Hypothesis:  [[0.03617394]\n",
      " [0.16561499]\n",
      " [0.3298952 ]\n",
      " [0.7701705 ]\n",
      " [0.932291  ]\n",
      " [0.977756  ]] \n",
      " Correct(Y):  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      " Accurary:  1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[1,2],[2,3],[3,1],[4,3],[5,3],[6,2]]\n",
    "y_data = [[0],[0],[0],[1],[1],[1]]\n",
    "\n",
    "# placeholder for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape = [None,2])  #입력 2\n",
    "Y = tf.placeholder(tf.float32, shape = [None,1])  #출력 1\n",
    "W = tf.Variable(tf.random_normal([2,1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1.,1.+tf.exp(tf.matmul(X,W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W)+b)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)\n",
    "\n",
    "# Accurary computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)  # 1/0\n",
    "accurary = tf.reduce_mean(tf.cast(tf.equal(predicted,Y), dtype = tf.float32))  #예측값과 실제값이 일치하는지\n",
    "\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initiallize Tensorflow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost,train], feed_dict = {X:x_data, Y:y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, \"cost:\",cost_val)\n",
    "            \n",
    "    # Accurary report\n",
    "    h, c, a = sess.run([hypothesis,predicted,accurary], feed_dict = {X:x_data,Y:y_data})\n",
    "    print(\"\\n Hypothesis: \",h,\"\\n Correct(Y): \",c, \"\\n Accurary: \",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<결과>\n",
    "\n",
    "step이 늘어날수록 cost값이 감소하는 것을 볼 수 있다.\n",
    "\n",
    "hypothesis이 0.5를 기준으로 0과 1로 나눈 값이 predicted(예측값) - 분류\n",
    "\n",
    "정확도는 1로 예측 정확도가 매우 높다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p8 Classifying diabets(당뇨병 분류)_ Logistic classification(binary)(이중 분류)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost: 0.93550336\n",
      "200 cost: 0.71667624\n",
      "400 cost: 0.6677714\n",
      "600 cost: 0.6478571\n",
      "800 cost: 0.6340215\n",
      "1000 cost: 0.62212497\n",
      "1200 cost: 0.61132133\n",
      "1400 cost: 0.60139567\n",
      "1600 cost: 0.59225976\n",
      "1800 cost: 0.5838526\n",
      "2000 cost: 0.5761199\n",
      "2200 cost: 0.56901073\n",
      "2400 cost: 0.5624767\n",
      "2600 cost: 0.5564721\n",
      "2800 cost: 0.5509538\n",
      "3000 cost: 0.54588145\n",
      "3200 cost: 0.54121745\n",
      "3400 cost: 0.53692657\n",
      "3600 cost: 0.53297657\n",
      "3800 cost: 0.5293374\n",
      "4000 cost: 0.52598166\n",
      "4200 cost: 0.5228842\n",
      "4400 cost: 0.52002203\n",
      "4600 cost: 0.5173744\n",
      "4800 cost: 0.5149223\n",
      "5000 cost: 0.5126485\n",
      "5200 cost: 0.51053745\n",
      "5400 cost: 0.5085752\n",
      "5600 cost: 0.5067489\n",
      "5800 cost: 0.5050469\n",
      "6000 cost: 0.50345904\n",
      "6200 cost: 0.5019758\n",
      "6400 cost: 0.50058866\n",
      "6600 cost: 0.4992899\n",
      "6800 cost: 0.49807242\n",
      "7000 cost: 0.49692997\n",
      "7200 cost: 0.49585664\n",
      "7400 cost: 0.49484727\n",
      "7600 cost: 0.49389705\n",
      "7800 cost: 0.49300158\n",
      "8000 cost: 0.49215683\n",
      "8200 cost: 0.49135923\n",
      "8400 cost: 0.4906054\n",
      "8600 cost: 0.48989236\n",
      "8800 cost: 0.48921728\n",
      "9000 cost: 0.48857746\n",
      "9200 cost: 0.48797062\n",
      "9400 cost: 0.48739463\n",
      "9600 cost: 0.48684746\n",
      "9800 cost: 0.4863273\n",
      "10000 cost: 0.48583224\n",
      "\n",
      "hypothesis:  [[0.39720294]\n",
      " [0.91146743]\n",
      " [0.22903425]\n",
      " [0.94003963]\n",
      " [0.30327976]\n",
      " [0.7469144 ]\n",
      " [0.9509233 ]\n",
      " [0.63565147]\n",
      " [0.21269125]\n",
      " [0.5035911 ]\n",
      " [0.6826056 ]\n",
      " [0.18940428]\n",
      " [0.2900034 ]\n",
      " [0.19322434]\n",
      " [0.76975286]\n",
      " [0.51439375]\n",
      " [0.7333877 ]\n",
      " [0.90490305]\n",
      " [0.8242192 ]\n",
      " [0.59408695]\n",
      " [0.68017423]\n",
      " [0.09943172]\n",
      " [0.6170315 ]\n",
      " [0.6808442 ]\n",
      " [0.3761025 ]\n",
      " [0.92302376]\n",
      " [0.51279277]\n",
      " [0.62046635]\n",
      " [0.6771476 ]\n",
      " [0.43498316]\n",
      " [0.94675744]\n",
      " [0.84441257]\n",
      " [0.55192554]\n",
      " [0.8426485 ]\n",
      " [0.38482475]\n",
      " [0.65038574]\n",
      " [0.83145285]\n",
      " [0.63544714]\n",
      " [0.4464829 ]\n",
      " [0.38675755]\n",
      " [0.75381565]\n",
      " [0.13075048]\n",
      " [0.41217062]\n",
      " [0.0776355 ]\n",
      " [0.599864  ]\n",
      " [0.92019176]\n",
      " [0.7099037 ]\n",
      " [0.7437832 ]\n",
      " [0.9147881 ]\n",
      " [0.9332377 ]\n",
      " [0.9309848 ]\n",
      " [0.20914933]\n",
      " [0.38601595]\n",
      " [0.9543215 ]\n",
      " [0.21119246]\n",
      " [0.6023587 ]\n",
      " [0.16770568]\n",
      " [0.76360834]\n",
      " [0.88673073]\n",
      " [0.51651186]\n",
      " [0.9562501 ]\n",
      " [0.72871697]\n",
      " [0.68419325]\n",
      " [0.8191577 ]\n",
      " [0.5643639 ]\n",
      " [0.5935432 ]\n",
      " [0.94789195]\n",
      " [0.64991367]\n",
      " [0.86200994]\n",
      " [0.6831787 ]\n",
      " [0.29354805]\n",
      " [0.65532804]\n",
      " [0.902039  ]\n",
      " [0.91099656]\n",
      " [0.88903636]\n",
      " [0.7800262 ]\n",
      " [0.47280863]\n",
      " [0.82735336]\n",
      " [0.8695167 ]\n",
      " [0.92504156]\n",
      " [0.87504995]\n",
      " [0.8002728 ]\n",
      " [0.3387372 ]\n",
      " [0.81750584]\n",
      " [0.58410275]\n",
      " [0.87763596]\n",
      " [0.46061584]\n",
      " [0.86021435]\n",
      " [0.9435079 ]\n",
      " [0.7294628 ]\n",
      " [0.85031915]\n",
      " [0.6529653 ]\n",
      " [0.68394506]\n",
      " [0.5584599 ]\n",
      " [0.88790965]\n",
      " [0.9761191 ]\n",
      " [0.91128355]\n",
      " [0.62907034]\n",
      " [0.27807426]\n",
      " [0.6385969 ]\n",
      " [0.57787156]\n",
      " [0.9541095 ]\n",
      " [0.8117814 ]\n",
      " [0.7851088 ]\n",
      " [0.82983536]\n",
      " [0.69323164]\n",
      " [0.9360438 ]\n",
      " [0.81960154]\n",
      " [0.47796023]\n",
      " [0.32572138]\n",
      " [0.93313706]\n",
      " [0.88942105]\n",
      " [0.46651366]\n",
      " [0.4189902 ]\n",
      " [0.63559335]\n",
      " [0.8753694 ]\n",
      " [0.8475604 ]\n",
      " [0.9092692 ]\n",
      " [0.2131966 ]\n",
      " [0.7256096 ]\n",
      " [0.85353017]\n",
      " [0.6131573 ]\n",
      " [0.63835704]\n",
      " [0.8915572 ]\n",
      " [0.72170615]\n",
      " [0.85721004]\n",
      " [0.7849725 ]\n",
      " [0.6347016 ]\n",
      " [0.44722956]\n",
      " [0.4938336 ]\n",
      " [0.4361537 ]\n",
      " [0.7725303 ]\n",
      " [0.92517793]\n",
      " [0.82062113]\n",
      " [0.81334686]\n",
      " [0.86700773]\n",
      " [0.46725643]\n",
      " [0.78246367]\n",
      " [0.7162606 ]\n",
      " [0.69387186]\n",
      " [0.89329815]\n",
      " [0.66741633]\n",
      " [0.5507972 ]\n",
      " [0.6748731 ]\n",
      " [0.9027363 ]\n",
      " [0.7954371 ]\n",
      " [0.4534186 ]\n",
      " [0.90310395]\n",
      " [0.66495717]\n",
      " [0.7850449 ]\n",
      " [0.27337116]\n",
      " [0.36097878]\n",
      " [0.11867648]\n",
      " [0.24118596]\n",
      " [0.9155408 ]\n",
      " [0.8776461 ]\n",
      " [0.9346632 ]\n",
      " [0.15748045]\n",
      " [0.45186535]\n",
      " [0.80525446]\n",
      " [0.60965323]\n",
      " [0.8461416 ]\n",
      " [0.42131025]\n",
      " [0.79482317]\n",
      " [0.55556834]\n",
      " [0.6644818 ]\n",
      " [0.74441874]\n",
      " [0.82686615]\n",
      " [0.7598485 ]\n",
      " [0.62339664]\n",
      " [0.8747808 ]\n",
      " [0.9157213 ]\n",
      " [0.95301586]\n",
      " [0.2567224 ]\n",
      " [0.82454836]\n",
      " [0.2825629 ]\n",
      " [0.40404156]\n",
      " [0.42572248]\n",
      " [0.8716618 ]\n",
      " [0.6852547 ]\n",
      " [0.91754067]\n",
      " [0.91750467]\n",
      " [0.5833534 ]\n",
      " [0.12934881]\n",
      " [0.18119678]\n",
      " [0.6256304 ]\n",
      " [0.76193553]\n",
      " [0.6221574 ]\n",
      " [0.8355515 ]\n",
      " [0.6751113 ]\n",
      " [0.37308103]\n",
      " [0.28225404]\n",
      " [0.88641715]\n",
      " [0.3866629 ]\n",
      " [0.8734955 ]\n",
      " [0.8915086 ]\n",
      " [0.78189015]\n",
      " [0.6129475 ]\n",
      " [0.5687059 ]\n",
      " [0.5533122 ]\n",
      " [0.6778516 ]\n",
      " [0.93522894]\n",
      " [0.7831446 ]\n",
      " [0.7745521 ]\n",
      " [0.12803751]\n",
      " [0.2742103 ]\n",
      " [0.91191804]\n",
      " [0.17368239]\n",
      " [0.92079693]\n",
      " [0.29118687]\n",
      " [0.19677866]\n",
      " [0.50173765]\n",
      " [0.7030636 ]\n",
      " [0.25225967]\n",
      " [0.79799235]\n",
      " [0.7312977 ]\n",
      " [0.79052067]\n",
      " [0.698571  ]\n",
      " [0.15375683]\n",
      " [0.3407064 ]\n",
      " [0.72381175]\n",
      " [0.564447  ]\n",
      " [0.91677535]\n",
      " [0.9334352 ]\n",
      " [0.70711696]\n",
      " [0.3904126 ]\n",
      " [0.03887683]\n",
      " [0.6755454 ]\n",
      " [0.36431527]\n",
      " [0.452009  ]\n",
      " [0.9458519 ]\n",
      " [0.655836  ]\n",
      " [0.9515611 ]\n",
      " [0.24201596]\n",
      " [0.11542296]\n",
      " [0.22482139]\n",
      " [0.75457144]\n",
      " [0.8973466 ]\n",
      " [0.8915629 ]\n",
      " [0.66766036]\n",
      " [0.7097666 ]\n",
      " [0.58505535]\n",
      " [0.1148974 ]\n",
      " [0.57859164]\n",
      " [0.10508174]\n",
      " [0.56265634]\n",
      " [0.80281377]\n",
      " [0.72454935]\n",
      " [0.674911  ]\n",
      " [0.9357166 ]\n",
      " [0.804798  ]\n",
      " [0.7580335 ]\n",
      " [0.7610551 ]\n",
      " [0.7794609 ]\n",
      " [0.81652415]\n",
      " [0.3381614 ]\n",
      " [0.35988715]\n",
      " [0.55063045]\n",
      " [0.8049413 ]\n",
      " [0.55899686]\n",
      " [0.70402396]\n",
      " [0.82082355]\n",
      " [0.38272643]\n",
      " [0.53317916]\n",
      " [0.68933153]\n",
      " [0.62900364]\n",
      " [0.44620043]\n",
      " [0.9099706 ]\n",
      " [0.7448295 ]\n",
      " [0.9450785 ]\n",
      " [0.59729904]\n",
      " [0.83766246]\n",
      " [0.790311  ]\n",
      " [0.8254074 ]\n",
      " [0.69011235]\n",
      " [0.83583784]\n",
      " [0.37461483]\n",
      " [0.590577  ]\n",
      " [0.6176851 ]\n",
      " [0.33530277]\n",
      " [0.80059636]\n",
      " [0.26842135]\n",
      " [0.6309183 ]\n",
      " [0.93441236]\n",
      " [0.8169683 ]\n",
      " [0.8608644 ]\n",
      " [0.74504316]\n",
      " [0.54116905]\n",
      " [0.7312895 ]\n",
      " [0.44322506]\n",
      " [0.50147045]\n",
      " [0.6166297 ]\n",
      " [0.5991186 ]\n",
      " [0.6564216 ]\n",
      " [0.6477008 ]\n",
      " [0.2416685 ]\n",
      " [0.70152664]\n",
      " [0.9175676 ]\n",
      " [0.49284163]\n",
      " [0.593241  ]\n",
      " [0.7766621 ]\n",
      " [0.45394173]\n",
      " [0.6809457 ]\n",
      " [0.52141297]\n",
      " [0.71413106]\n",
      " [0.8877801 ]\n",
      " [0.7092371 ]\n",
      " [0.64699167]\n",
      " [0.86162317]\n",
      " [0.5484601 ]\n",
      " [0.8507679 ]\n",
      " [0.92118067]\n",
      " [0.28850764]\n",
      " [0.8098777 ]\n",
      " [0.22342384]\n",
      " [0.74254245]\n",
      " [0.7735347 ]\n",
      " [0.63677245]\n",
      " [0.36397985]\n",
      " [0.7601818 ]\n",
      " [0.6805916 ]\n",
      " [0.76925313]\n",
      " [0.1910539 ]\n",
      " [0.81967604]\n",
      " [0.85824406]\n",
      " [0.5406416 ]\n",
      " [0.9418664 ]\n",
      " [0.283651  ]\n",
      " [0.6880051 ]\n",
      " [0.94455516]\n",
      " [0.24309799]\n",
      " [0.5453062 ]\n",
      " [0.69077384]\n",
      " [0.34974158]\n",
      " [0.17525673]\n",
      " [0.81190467]\n",
      " [0.91089684]\n",
      " [0.8448574 ]\n",
      " [0.60542077]\n",
      " [0.70849156]\n",
      " [0.5882195 ]\n",
      " [0.78343976]\n",
      " [0.8146925 ]\n",
      " [0.92763865]\n",
      " [0.719592  ]\n",
      " [0.77649266]\n",
      " [0.5595638 ]\n",
      " [0.9223397 ]\n",
      " [0.93685484]\n",
      " [0.76311874]\n",
      " [0.25427428]\n",
      " [0.7573062 ]\n",
      " [0.37994784]\n",
      " [0.80706394]\n",
      " [0.20215276]\n",
      " [0.24125496]\n",
      " [0.42948893]\n",
      " [0.6370075 ]\n",
      " [0.39950156]\n",
      " [0.5275324 ]\n",
      " [0.87069917]\n",
      " [0.6516016 ]\n",
      " [0.8150605 ]\n",
      " [0.93326193]\n",
      " [0.6948862 ]\n",
      " [0.1102263 ]\n",
      " [0.55193293]\n",
      " [0.8648107 ]\n",
      " [0.8851557 ]\n",
      " [0.7535689 ]\n",
      " [0.30815333]\n",
      " [0.8548713 ]\n",
      " [0.9188597 ]\n",
      " [0.31902674]\n",
      " [0.6589989 ]\n",
      " [0.85069   ]\n",
      " [0.8023001 ]\n",
      " [0.8539641 ]\n",
      " [0.91066587]\n",
      " [0.8544655 ]\n",
      " [0.9095224 ]\n",
      " [0.6533811 ]\n",
      " [0.5689025 ]\n",
      " [0.51100147]\n",
      " [0.84888494]\n",
      " [0.8800429 ]\n",
      " [0.2473709 ]\n",
      " [0.75072813]\n",
      " [0.8486686 ]\n",
      " [0.33529964]\n",
      " [0.64333856]\n",
      " [0.89133334]\n",
      " [0.5481415 ]\n",
      " [0.9108826 ]\n",
      " [0.2293616 ]\n",
      " [0.8457454 ]\n",
      " [0.6356319 ]\n",
      " [0.85520184]\n",
      " [0.37031984]\n",
      " [0.7564058 ]\n",
      " [0.72783923]\n",
      " [0.80182743]\n",
      " [0.10637918]\n",
      " [0.2312237 ]\n",
      " [0.6576525 ]\n",
      " [0.8187854 ]\n",
      " [0.42983714]\n",
      " [0.76120776]\n",
      " [0.54919916]\n",
      " [0.28983814]\n",
      " [0.8230848 ]\n",
      " [0.4189721 ]\n",
      " [0.9197811 ]\n",
      " [0.81147826]\n",
      " [0.68494326]\n",
      " [0.91485476]\n",
      " [0.7573097 ]\n",
      " [0.76945066]\n",
      " [0.34900153]\n",
      " [0.26678786]\n",
      " [0.7831925 ]\n",
      " [0.42404118]\n",
      " [0.4794377 ]\n",
      " [0.8831352 ]\n",
      " [0.8954084 ]\n",
      " [0.9042313 ]\n",
      " [0.941448  ]\n",
      " [0.66395247]\n",
      " [0.8590953 ]\n",
      " [0.41583505]\n",
      " [0.3283781 ]\n",
      " [0.47508177]\n",
      " [0.9111806 ]\n",
      " [0.60718364]\n",
      " [0.11494881]\n",
      " [0.93266577]\n",
      " [0.8177139 ]\n",
      " [0.6218784 ]\n",
      " [0.75278634]\n",
      " [0.05166265]\n",
      " [0.9060486 ]\n",
      " [0.80490196]\n",
      " [0.7995867 ]\n",
      " [0.746274  ]\n",
      " [0.95763564]\n",
      " [0.63732576]\n",
      " [0.7984641 ]\n",
      " [0.7589574 ]\n",
      " [0.87957036]\n",
      " [0.18264562]\n",
      " [0.67197335]\n",
      " [0.9138018 ]\n",
      " [0.63624394]\n",
      " [0.74522096]\n",
      " [0.94587815]\n",
      " [0.8860477 ]\n",
      " [0.85623795]\n",
      " [0.49685737]\n",
      " [0.7769091 ]\n",
      " [0.9433925 ]\n",
      " [0.7980645 ]\n",
      " [0.6701335 ]\n",
      " [0.36006707]\n",
      " [0.45120963]\n",
      " [0.50763166]\n",
      " [0.595787  ]\n",
      " [0.49556231]\n",
      " [0.7827537 ]\n",
      " [0.49914715]\n",
      " [0.80098784]\n",
      " [0.8007683 ]\n",
      " [0.7474429 ]\n",
      " [0.5877812 ]\n",
      " [0.4633064 ]\n",
      " [0.53783   ]\n",
      " [0.9390527 ]\n",
      " [0.8458476 ]\n",
      " [0.31722534]\n",
      " [0.4880476 ]\n",
      " [0.5257402 ]\n",
      " [0.1283598 ]\n",
      " [0.84572303]\n",
      " [0.14406398]\n",
      " [0.9149612 ]\n",
      " [0.84057534]\n",
      " [0.82455456]\n",
      " [0.68737996]\n",
      " [0.9012749 ]\n",
      " [0.35352278]\n",
      " [0.7782143 ]\n",
      " [0.93293124]\n",
      " [0.2925315 ]\n",
      " [0.4430885 ]\n",
      " [0.8381386 ]\n",
      " [0.862028  ]\n",
      " [0.69051224]\n",
      " [0.8389261 ]\n",
      " [0.8019018 ]\n",
      " [0.81191075]\n",
      " [0.24635145]\n",
      " [0.76660776]\n",
      " [0.9219518 ]\n",
      " [0.6248272 ]\n",
      " [0.77956456]\n",
      " [0.6714252 ]\n",
      " [0.8224683 ]\n",
      " [0.8766694 ]\n",
      " [0.9292004 ]\n",
      " [0.56245184]\n",
      " [0.41290796]\n",
      " [0.8142154 ]\n",
      " [0.6468847 ]\n",
      " [0.96722984]\n",
      " [0.756479  ]\n",
      " [0.72204924]\n",
      " [0.4536514 ]\n",
      " [0.73334163]\n",
      " [0.90553415]\n",
      " [0.9431747 ]\n",
      " [0.8639863 ]\n",
      " [0.713199  ]\n",
      " [0.68748367]\n",
      " [0.8079436 ]\n",
      " [0.52885133]\n",
      " [0.87980676]\n",
      " [0.82507825]\n",
      " [0.9234935 ]\n",
      " [0.63691187]\n",
      " [0.68515986]\n",
      " [0.91689235]\n",
      " [0.5153361 ]\n",
      " [0.570434  ]\n",
      " [0.7090538 ]\n",
      " [0.71346956]\n",
      " [0.65077424]\n",
      " [0.89264727]\n",
      " [0.9211699 ]\n",
      " [0.18609461]\n",
      " [0.1506837 ]\n",
      " [0.7374794 ]\n",
      " [0.53693223]\n",
      " [0.15607867]\n",
      " [0.80528104]\n",
      " [0.9123413 ]\n",
      " [0.6786257 ]\n",
      " [0.93907034]\n",
      " [0.9215031 ]\n",
      " [0.7480491 ]\n",
      " [0.8356677 ]\n",
      " [0.69901204]\n",
      " [0.59813255]\n",
      " [0.77906823]\n",
      " [0.6128667 ]\n",
      " [0.12754643]\n",
      " [0.9161198 ]\n",
      " [0.8855535 ]\n",
      " [0.7302227 ]\n",
      " [0.9184717 ]\n",
      " [0.88235176]\n",
      " [0.8953282 ]\n",
      " [0.57540447]\n",
      " [0.70516235]\n",
      " [0.89299357]\n",
      " [0.67158395]\n",
      " [0.87389433]\n",
      " [0.9056262 ]\n",
      " [0.5379352 ]\n",
      " [0.8645302 ]\n",
      " [0.85135674]\n",
      " [0.5831665 ]\n",
      " [0.5370501 ]\n",
      " [0.16252947]\n",
      " [0.25710464]\n",
      " [0.805745  ]\n",
      " [0.56326145]\n",
      " [0.6975549 ]\n",
      " [0.48540315]\n",
      " [0.9133252 ]\n",
      " [0.46421522]\n",
      " [0.8140447 ]\n",
      " [0.26700366]\n",
      " [0.90405333]\n",
      " [0.37711224]\n",
      " [0.80483377]\n",
      " [0.5743835 ]\n",
      " [0.8930423 ]\n",
      " [0.6319367 ]\n",
      " [0.1929493 ]\n",
      " [0.8219546 ]\n",
      " [0.95695233]\n",
      " [0.36882338]\n",
      " [0.9247143 ]\n",
      " [0.8042548 ]\n",
      " [0.85686886]\n",
      " [0.770028  ]\n",
      " [0.43511623]\n",
      " [0.32281187]\n",
      " [0.6975716 ]\n",
      " [0.20342645]\n",
      " [0.94869006]\n",
      " [0.34450233]\n",
      " [0.90380764]\n",
      " [0.88155556]\n",
      " [0.4193116 ]\n",
      " [0.20687872]\n",
      " [0.66480356]\n",
      " [0.45671734]\n",
      " [0.8262277 ]\n",
      " [0.6581165 ]\n",
      " [0.97609884]\n",
      " [0.5189544 ]\n",
      " [0.66042936]\n",
      " [0.7860024 ]\n",
      " [0.77645415]\n",
      " [0.08192092]\n",
      " [0.78323424]\n",
      " [0.8180191 ]\n",
      " [0.86399484]\n",
      " [0.6412947 ]\n",
      " [0.4779793 ]\n",
      " [0.5960656 ]\n",
      " [0.8797076 ]\n",
      " [0.66881394]\n",
      " [0.7975872 ]\n",
      " [0.8239665 ]\n",
      " [0.8056327 ]\n",
      " [0.81642735]\n",
      " [0.604344  ]\n",
      " [0.79032826]\n",
      " [0.89241064]\n",
      " [0.74865335]\n",
      " [0.94080937]\n",
      " [0.7694986 ]\n",
      " [0.63015944]\n",
      " [0.4722074 ]\n",
      " [0.85121465]\n",
      " [0.84965074]\n",
      " [0.43780714]\n",
      " [0.58794826]\n",
      " [0.29250228]\n",
      " [0.5535421 ]\n",
      " [0.7818564 ]\n",
      " [0.9483424 ]\n",
      " [0.84115696]\n",
      " [0.7523656 ]\n",
      " [0.77837455]\n",
      " [0.88163376]\n",
      " [0.4706215 ]\n",
      " [0.933512  ]\n",
      " [0.63904357]\n",
      " [0.8664042 ]\n",
      " [0.29291803]\n",
      " [0.09651905]\n",
      " [0.2517738 ]\n",
      " [0.40542355]\n",
      " [0.71079034]\n",
      " [0.83564305]\n",
      " [0.5004295 ]\n",
      " [0.7010722 ]\n",
      " [0.8395891 ]\n",
      " [0.48810288]\n",
      " [0.38609618]\n",
      " [0.8746809 ]\n",
      " [0.8729613 ]\n",
      " [0.41802442]\n",
      " [0.701892  ]\n",
      " [0.18278643]\n",
      " [0.359559  ]\n",
      " [0.77970076]\n",
      " [0.74347425]\n",
      " [0.8878635 ]\n",
      " [0.9795897 ]\n",
      " [0.1973497 ]\n",
      " [0.78748167]\n",
      " [0.6067622 ]\n",
      " [0.45991215]\n",
      " [0.7057164 ]\n",
      " [0.7274802 ]\n",
      " [0.8741045 ]\n",
      " [0.6973539 ]\n",
      " [0.5334523 ]\n",
      " [0.6601153 ]\n",
      " [0.13656104]\n",
      " [0.71978116]\n",
      " [0.6166241 ]\n",
      " [0.9109651 ]\n",
      " [0.45536843]\n",
      " [0.51275736]\n",
      " [0.8054319 ]\n",
      " [0.6609143 ]\n",
      " [0.5347258 ]\n",
      " [0.7381549 ]\n",
      " [0.60450494]\n",
      " [0.2800101 ]\n",
      " [0.6477791 ]\n",
      " [0.8622298 ]\n",
      " [0.82729167]\n",
      " [0.6310178 ]\n",
      " [0.7794358 ]\n",
      " [0.28528965]\n",
      " [0.867939  ]\n",
      " [0.51270753]\n",
      " [0.7534995 ]\n",
      " [0.4463549 ]\n",
      " [0.6344486 ]\n",
      " [0.82878137]\n",
      " [0.19092524]\n",
      " [0.31433138]\n",
      " [0.76719075]\n",
      " [0.8519058 ]\n",
      " [0.7685369 ]\n",
      " [0.86609805]\n",
      " [0.809396  ]\n",
      " [0.68913823]\n",
      " [0.7234771 ]\n",
      " [0.7421239 ]\n",
      " [0.6864775 ]\n",
      " [0.7836584 ]\n",
      " [0.41567424]\n",
      " [0.3774693 ]\n",
      " [0.8981312 ]\n",
      " [0.77040476]\n",
      " [0.615165  ]\n",
      " [0.29468292]\n",
      " [0.8841717 ]\n",
      " [0.81336004]\n",
      " [0.832924  ]\n",
      " [0.644114  ]\n",
      " [0.87287617]\n",
      " [0.8715664 ]\n",
      " [0.78732675]\n",
      " [0.48012826]\n",
      " [0.9061334 ]\n",
      " [0.90377116]\n",
      " [0.32361767]\n",
      " [0.19178441]\n",
      " [0.73523515]\n",
      " [0.37172833]\n",
      " [0.85984015]\n",
      " [0.30023354]\n",
      " [0.45426667]\n",
      " [0.47188753]\n",
      " [0.7891944 ]\n",
      " [0.8392584 ]\n",
      " [0.12483889]\n",
      " [0.33949357]\n",
      " [0.6346917 ]\n",
      " [0.45401597]\n",
      " [0.5475115 ]\n",
      " [0.7887356 ]\n",
      " [0.14598745]\n",
      " [0.9295131 ]\n",
      " [0.18018204]\n",
      " [0.85633457]\n",
      " [0.7631052 ]\n",
      " [0.69406015]\n",
      " [0.8084515 ]\n",
      " [0.7361711 ]\n",
      " [0.87691927]] \n",
      "Correct(Y):  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accurary:  0.770751\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt('data-03-diabetes.csv',delimiter = ',', dtype = np.float32)\n",
    "x_data = xy[:,0:-1]\n",
    "y_data = xy[:,[-1]]\n",
    "\n",
    "# placeholders for a tensor that will be always ped.\n",
    "X = tf.placeholder(tf.float32, shape = [None,8])\n",
    "Y = tf.placeholder(tf.float32, shape = [None,1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([8,1]),name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]),name = 'bias')\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1.,1.+tf.exp(tf.matmul(X,W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W)+b)\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)\n",
    "\n",
    "# Accurary computation\n",
    "# True if hypothesis > 0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accurary = tf.reduce_mean(tf.cast(tf.equal(predicted,Y), dtype = tf.float32))\n",
    "\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    feed = {X:x_data,Y:y_data}\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict = feed)\n",
    "        if step % 200 == 0:\n",
    "            print(step, \"cost:\",sess.run(cost, feed_dict = feed))\n",
    "            \n",
    "    # Accurary report\n",
    "    h,c,a = sess.run([hypothesis, predicted,accurary], feed_dict = feed)\n",
    "    print(\"\\nhypothesis: \",h, \"\\nCorrect(Y): \",c, \"\\nAccurary: \",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<결과>\n",
    "\n",
    "정확도는 약 77.07%로 모델의 예측 정확도가 높지 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p15 Animal classification(동물분류)_Multi-Class Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:    0\tLoss:7.957\tAcc:19.80%\n",
      "Step:  100\tLoss:2.225\tAcc:40.59%\n",
      "Step:  200\tLoss:1.335\tAcc:57.43%\n",
      "Step:  300\tLoss:1.018\tAcc:74.26%\n",
      "Step:  400\tLoss:0.858\tAcc:78.22%\n",
      "Step:  500\tLoss:0.782\tAcc:79.21%\n",
      "Step:  600\tLoss:0.733\tAcc:79.21%\n",
      "Step:  700\tLoss:0.693\tAcc:81.19%\n",
      "Step:  800\tLoss:0.658\tAcc:81.19%\n",
      "Step:  900\tLoss:0.627\tAcc:81.19%\n",
      "Step: 1000\tLoss:0.599\tAcc:83.17%\n",
      "Step: 1100\tLoss:0.573\tAcc:83.17%\n",
      "Step: 1200\tLoss:0.550\tAcc:83.17%\n",
      "Step: 1300\tLoss:0.528\tAcc:83.17%\n",
      "Step: 1400\tLoss:0.508\tAcc:83.17%\n",
      "Step: 1500\tLoss:0.489\tAcc:84.16%\n",
      "Step: 1600\tLoss:0.471\tAcc:84.16%\n",
      "Step: 1700\tLoss:0.454\tAcc:84.16%\n",
      "Step: 1800\tLoss:0.439\tAcc:84.16%\n",
      "Step: 1900\tLoss:0.424\tAcc:86.14%\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:3 True Y: 3\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:3 True Y: 3\n",
      "[True] Prediction:3 True Y: 3\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:3 True Y: 3\n",
      "[True] Prediction:6 True Y: 6\n",
      "[True] Prediction:6 True Y: 6\n",
      "[True] Prediction:6 True Y: 6\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:3 True Y: 3\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:1 True Y: 1\n",
      "[False] Prediction:4 True Y: 5\n",
      "[False] Prediction:2 True Y: 4\n",
      "[False] Prediction:5 True Y: 4\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:5 True Y: 5\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:3 True Y: 3\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:3 True Y: 3\n",
      "[True] Prediction:5 True Y: 5\n",
      "[True] Prediction:5 True Y: 5\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:5 True Y: 5\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:6 True Y: 6\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:5 True Y: 5\n",
      "[False] Prediction:2 True Y: 4\n",
      "[True] Prediction:6 True Y: 6\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:3 True Y: 3\n",
      "[True] Prediction:3 True Y: 3\n",
      "[False] Prediction:3 True Y: 2\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:6 True Y: 6\n",
      "[True] Prediction:3 True Y: 3\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[False] Prediction:0 True Y: 2\n",
      "[True] Prediction:6 True Y: 6\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:1 True Y: 1\n",
      "[False] Prediction:3 True Y: 2\n",
      "[False] Prediction:3 True Y: 6\n",
      "[True] Prediction:3 True Y: 3\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:6 True Y: 6\n",
      "[True] Prediction:3 True Y: 3\n",
      "[True] Prediction:1 True Y: 1\n",
      "[False] Prediction:4 True Y: 5\n",
      "[False] Prediction:5 True Y: 4\n",
      "[False] Prediction:6 True Y: 2\n",
      "[False] Prediction:0 True Y: 2\n",
      "[True] Prediction:3 True Y: 3\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:5 True Y: 5\n",
      "[True] Prediction:0 True Y: 0\n",
      "[False] Prediction:3 True Y: 6\n",
      "[True] Prediction:1 True Y: 1\n"
     ]
    }
   ],
   "source": [
    "#다시 정확도 너무 낮음\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Predicting animal type based on various features\n",
    "xy = np.loadtxt('data-04-zoo.csv',delimiter = ',', dtype = np.float32)\n",
    "x_data = xy[:,0:-1]\n",
    "y_data = xy[:,[-1]]\n",
    "\n",
    "nb_classes = 7  # calss : 0~6\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None,16])\n",
    "Y = tf.placeholder(tf.int32,[None,1])  # 0~6\n",
    "\n",
    "Y_one_hot = tf.one_hot(Y,nb_classes)  #one hot\n",
    "Y_one_hot = tf.reshape(Y_one_hot,[-1,nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([16, nb_classes]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]),name = 'bias')\n",
    "\n",
    "# tf.nn.softmax computes sotfmax activations\n",
    "# softmax = exp(logits)/reduce_sum(exp(logits),dim)\n",
    "logits = tf.matmul(X,W)+b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y_one_hot)\n",
    "\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "prediction = tf.argmax(hypothesis,1)\n",
    "correct_prediction = tf.equal(prediction,tf.argmax(Y_one_hot,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(2000):\n",
    "        sess.run(optimizer,feed_dict={X:x_data,Y:y_data})\n",
    "        if step % 100 == 0:\n",
    "            loss, acc = sess.run([cost,accuracy],feed_dict = {X:x_data,Y:y_data})\n",
    "            print(\"Step:{:5}\\tLoss:{:.3f}\\tAcc:{:.2%}\".format(step,loss,acc))\n",
    "    \n",
    "    # Let's we see if we can predict\n",
    "    pred = sess.run(prediction, feed_dict={X:x_data,Y:y_data})\n",
    "    # y_data: (N,1) = flatten => (N,) matches pred.shape\n",
    "    for p,y in zip(pred, y_data.flatten()):\n",
    "        print(\"[{}] Prediction:{} True Y: {}\".format(p==int(y),p,int(y)))                \n",
    "        # p==int(y) : 같으면 True 아니면 False를 반환\n",
    "        # p:예측값 int(y): 실제값\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<결과> \n",
    "\n",
    "step이 늘어날수록 cost값인 loss는 줄어들고, 정확도는 높아지는 것을 볼 수 있다.\n",
    "\n",
    "예측값과 실제값, 값의 정답 유무 또한 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Predicting animal type based on various features\n",
    "xy = np.loadtxt('data-04-zoo.csv',delimiter = ',', dtype = np.float32)\n",
    "x_data = xy[:,0:-1]\n",
    "y_data = xy[:,[-1]]\n",
    "\n",
    "nb_classes = 7  # calss : 0~6\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None,16])\n",
    "Y = tf.placeholder(tf.int32,[None,1])  # 0~6\n",
    "\n",
    "Y_one_hot = tf.one_hot(Y,nb_classes)  #one hot\n",
    "Y_one_hot = tf.reshape(Y_one_hot,[-1,nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([16, nb_classes]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]),name = 'bias')\n",
    "\n",
    "# tf.nn.softmax computes sotfmax activations\n",
    "# softmax = exp(logits)/reduce_sum(exp(logits),dim)\n",
    "logits = tf.matmul(X,W)+b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y_one_hot)\n",
    "\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "prediction = tf.argmax(hypothesis,1)\n",
    "correct_prediction = tf.equal(prediction,tf.argmax(Y_one_hot,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:    0\tLoss:5.246\tAcc:2.97%\n",
      "Step:  100\tLoss:2.280\tAcc:35.64%\n",
      "Step:  200\tLoss:1.741\tAcc:46.53%\n",
      "Step:  300\tLoss:1.401\tAcc:47.52%\n",
      "Step:  400\tLoss:1.160\tAcc:52.48%\n",
      "Step:  500\tLoss:0.995\tAcc:68.32%\n",
      "Step:  600\tLoss:0.883\tAcc:78.22%\n",
      "Step:  700\tLoss:0.804\tAcc:80.20%\n",
      "Step:  800\tLoss:0.744\tAcc:81.19%\n",
      "Step:  900\tLoss:0.695\tAcc:82.18%\n",
      "Step: 1000\tLoss:0.655\tAcc:82.18%\n",
      "Step: 1100\tLoss:0.619\tAcc:82.18%\n",
      "Step: 1200\tLoss:0.588\tAcc:82.18%\n",
      "Step: 1300\tLoss:0.561\tAcc:82.18%\n",
      "Step: 1400\tLoss:0.536\tAcc:82.18%\n",
      "Step: 1500\tLoss:0.513\tAcc:84.16%\n",
      "Step: 1600\tLoss:0.492\tAcc:85.15%\n",
      "Step: 1700\tLoss:0.473\tAcc:87.13%\n",
      "Step: 1800\tLoss:0.456\tAcc:89.11%\n",
      "Step: 1900\tLoss:0.440\tAcc:89.11%\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:3 True Y: 3\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:3 True Y: 3\n",
      "[True] Prediction:3 True Y: 3\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:3 True Y: 3\n",
      "[False] Prediction:2 True Y: 6\n",
      "[True] Prediction:6 True Y: 6\n",
      "[True] Prediction:6 True Y: 6\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:3 True Y: 3\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:5 True Y: 5\n",
      "[True] Prediction:4 True Y: 4\n",
      "[True] Prediction:4 True Y: 4\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:5 True Y: 5\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:3 True Y: 3\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:3 True Y: 3\n",
      "[True] Prediction:5 True Y: 5\n",
      "[True] Prediction:5 True Y: 5\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:5 True Y: 5\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:6 True Y: 6\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:5 True Y: 5\n",
      "[True] Prediction:4 True Y: 4\n",
      "[True] Prediction:6 True Y: 6\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:3 True Y: 3\n",
      "[True] Prediction:3 True Y: 3\n",
      "[False] Prediction:1 True Y: 2\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:1 True Y: 1\n",
      "[False] Prediction:5 True Y: 6\n",
      "[True] Prediction:3 True Y: 3\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[False] Prediction:3 True Y: 2\n",
      "[False] Prediction:2 True Y: 6\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:1 True Y: 1\n",
      "[False] Prediction:1 True Y: 2\n",
      "[False] Prediction:2 True Y: 6\n",
      "[True] Prediction:3 True Y: 3\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:6 True Y: 6\n",
      "[True] Prediction:3 True Y: 3\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:5 True Y: 5\n",
      "[False] Prediction:2 True Y: 4\n",
      "[False] Prediction:1 True Y: 2\n",
      "[False] Prediction:0 True Y: 2\n",
      "[True] Prediction:3 True Y: 3\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:1 True Y: 1\n",
      "[True] Prediction:0 True Y: 0\n",
      "[True] Prediction:5 True Y: 5\n",
      "[True] Prediction:0 True Y: 0\n",
      "[False] Prediction:2 True Y: 6\n",
      "[True] Prediction:1 True Y: 1\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(2000):\n",
    "        sess.run(optimizer,feed_dict={X:x_data,Y:y_data})\n",
    "        if step % 100 == 0:\n",
    "            loss, acc = sess.run([cost,accuracy],feed_dict = {X:x_data,Y:y_data})\n",
    "            print(\"Step:{:5}\\tLoss:{:.3f}\\tAcc:{:.2%}\".format(step,loss,acc))\n",
    "    \n",
    "    pred = sess.run(prediction,feed_dict={X:x_data,Y:y_data})\n",
    "    for p,y in zip(pred, y_data.flatten()):\n",
    "        print(\"[{}] Prediction:{} True Y: {}\".format(p==int(y),p,int(y)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
