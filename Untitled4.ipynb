{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1d convolution and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d Implementation [ 7  8  9 11  3]\n",
      "Numpy Results:     [ 7  8  9 11  3]\n"
     ]
    }
   ],
   "source": [
    "#1D convolution and padding\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def conv1d(x,w,p=0,s=1):\n",
    "    w_rot = np.array(w[::-1])\n",
    "    x_padded = np.array(x)\n",
    "    if p > 0:\n",
    "        zero_pad = np.zeros(shape = p)\n",
    "        x_padded = np.concatenate([zero_pad, x_padded. zero_pad])\n",
    "    res = []\n",
    "    for i in range(0,int((len(x)+2*p-len(w))/s)+1):\n",
    "        j = s*i;\n",
    "        res.append(np.sum(x_padded[j:j+w_rot.shape[0]]*w_rot))\n",
    "    return np.array(res)\n",
    "\n",
    "#Testing\n",
    "\n",
    "x = [1,0,2,3,0,1,1]\n",
    "w = [2,1,3]\n",
    "print('Conv1d Implementation', conv1d(x,w,p=0, s=1))\n",
    "print('Numpy Results:    ', np.convolve(x,w,mode='valid'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"i:0\", shape=(7,), dtype=float32) \n",
      " Tensor(\"k:0\", shape=(3,), dtype=float32) \\/\n",
      "Tensor(\"data:0\", shape=(1, 7, 1), dtype=float32) \n",
      " Tensor(\"kernel:0\", shape=(3, 1, 1), dtype=float32) \n",
      "\n",
      "[ 8. 11.  7.  9.  4.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "i = tf.constant([1,0,2,3,0,1,1], dtype = tf.float32, name = 'i')\n",
    "k = tf.constant([2,1,3], dtype = tf.float32, name = 'k')\n",
    "            \n",
    "print(i,'\\n',k,'\\/')\n",
    "                \n",
    "data = tf.reshape(i, [1,int(i.shape[0]),1], name = 'data')\n",
    "kernel = tf.reshape(k, [int(k.shape[0]),1,1], name = 'kernel')\n",
    "                \n",
    "print(data, '\\n', kernel, '\\n')\n",
    "\n",
    "res = tf.squeeze(tf.nn.conv1d(data, kernel, 1, 'VALID'))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-63d8b1c40712>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v1.train' has no attribute 'AdamOpimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-63d8b1c40712>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m \u001b[0mtrain_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOpimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[0mcorrect_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\module_wrapper.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    191\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m       \u001b[0mattr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfmw_wrapped_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfmw_public_apis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow._api.v1.train' has no attribute 'AdamOpimizer'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\",one_hot = True)\n",
    "\n",
    "def build_CNN_classifier(x):\n",
    "    x_image = tf.reshape(x, [-1,28,28,1]) # [batch,height,width,channel]]\n",
    "    #input에 들어올 이미지 데이터의 shape을 재설정\n",
    "\n",
    "\n",
    "    # 1st Convolution Layer\n",
    "    #5*5 Kernel Size, 32 Filters\n",
    "    #28*28*1 -> 28*28*32\n",
    "    W_conv1 = tf.Variable(tf.truncated_normal(shape = [5,5,1,32], stddev = 5e-2))\n",
    "    b_conv1 = tf.Variable(tf.constant(0.1, shape = [32]))\n",
    "    h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, strides = [1,1,1,1], padding = 'SAME') + b_conv1)\n",
    "\n",
    "\n",
    "    # 1st Pooling Layer\n",
    "    #Max Pooling : 1/2 down sampling\n",
    "    #28*28*32 -> 14*14*32\n",
    "    h_pool1 = tf.nn.max_pool(h_conv1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "\n",
    "    # 2st Convolution Layer\n",
    "    #5*5 kernel Size, 64 Filters\n",
    "    #14*14*32 -> 14*14*64\n",
    "    W_conv2 = tf.Variable(tf.truncated_normal(shape = [5,5,32,64], stddev = 5e-2))\n",
    "    b_conv2 = tf.Variable(tf.constant(0.1, shape = [64]))\n",
    "    h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, W_conv2, strides = [1,1,1,1], padding = 'SAME') + b_conv2)\n",
    "\n",
    "    \n",
    "    # 2nd Pooling Layer\n",
    "    #Max Pooling : 1/2 down sampling\n",
    "    # 14*14*64 -> 7*7*64\n",
    "    h_pool2 = tf.nn.max_pool(h_conv2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "\n",
    "\n",
    "    # Fully Connected Layer\n",
    "    #7*7 Size, 64 activation maps: 1024 features\n",
    "    #7*7*64(3136) -> 1024\n",
    "    W_fc1 = tf.Variable(tf.truncated_normal(shape = [7*7*64, 1024], stddev = 5e-2))\n",
    "    b_fc1 = tf.Variable(tf.constant(0.1, shape = [1024]))\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "\n",
    "    # Output Layer\n",
    "    #1024 features -> one of 10 one_hot encoding vactors\n",
    "    #1024 -> 10\n",
    "    W_output = tf.Variable(tf.truncated_normal(shape = [1024, 10], stddev = 5e-2))\n",
    "    b_output = tf.Variable(tf.constant(0.1, shape = [10]))\n",
    "    logits = tf.matmul(h_fc1, W_output) + b_output\n",
    "    y_pred = tf.nn.softmax(logits)\n",
    "\n",
    "    return y_pred, logits\n",
    "\n",
    "#Training and Test\n",
    "x = tf.placeholder(tf.float32, shape = [None,784])\n",
    "y = tf.placeholder(tf.float32, shape = [None,10]) # 0~9\n",
    "\n",
    "y_pred, logits = build_CNN_classifier(x)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = y))\n",
    "train_step = tf.train.AdamOpimizer(1e-4).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(100000):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict = {X:batchch[0],y:batch[1]})\n",
    "            print(\"Epoch: %d, Training accuracy: %f\"% (i, training_accuracy))\n",
    "\n",
    "        sess.run([train_step], feed_dict = {X:batch[0],y:batch[1]})\n",
    "        \n",
    "    print(\"Test accuracy: %f\" % accracy.eval(feed_dict = {X:mnist.test.images,y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Epoch: 0, Training accuracy: 0.164062, Loss: 175.232574\n",
      "Epoch: 100, Training accuracy: 0.117188, Loss: 2.160092\n",
      "Epoch: 200, Training accuracy: 0.296875, Loss: 2.050426\n",
      "Epoch: 300, Training accuracy: 0.367188, Loss: 1.928743\n",
      "Epoch: 400, Training accuracy: 0.429688, Loss: 1.695590\n",
      "Epoch: 500, Training accuracy: 0.429688, Loss: 1.613896\n",
      "Epoch: 600, Training accuracy: 0.507812, Loss: 1.307039\n",
      "Epoch: 700, Training accuracy: 0.468750, Loss: 1.461413\n",
      "Epoch: 800, Training accuracy: 0.515625, Loss: 1.409701\n",
      "Epoch: 900, Training accuracy: 0.554688, Loss: 1.219056\n",
      "Epoch: 1000, Training accuracy: 0.570312, Loss: 1.311036\n",
      "Epoch: 1100, Training accuracy: 0.515625, Loss: 1.360898\n",
      "Epoch: 1200, Training accuracy: 0.570312, Loss: 1.212081\n",
      "Epoch: 1300, Training accuracy: 0.523438, Loss: 1.175260\n",
      "Epoch: 1400, Training accuracy: 0.601562, Loss: 1.165669\n",
      "Epoch: 1500, Training accuracy: 0.679688, Loss: 0.848529\n",
      "Epoch: 1600, Training accuracy: 0.570312, Loss: 1.189741\n",
      "Epoch: 1700, Training accuracy: 0.617188, Loss: 1.207075\n",
      "Epoch: 1800, Training accuracy: 0.695312, Loss: 0.958322\n",
      "Epoch: 1900, Training accuracy: 0.703125, Loss: 0.949233\n",
      "Epoch: 2000, Training accuracy: 0.585938, Loss: 1.251205\n",
      "Epoch: 2100, Training accuracy: 0.640625, Loss: 1.096667\n",
      "Epoch: 2200, Training accuracy: 0.671875, Loss: 0.914613\n",
      "Epoch: 2300, Training accuracy: 0.695312, Loss: 1.065895\n",
      "Epoch: 2400, Training accuracy: 0.640625, Loss: 0.935165\n",
      "Epoch: 2500, Training accuracy: 0.664062, Loss: 0.819931\n",
      "Epoch: 2600, Training accuracy: 0.695312, Loss: 0.926330\n",
      "Epoch: 2700, Training accuracy: 0.648438, Loss: 1.065030\n",
      "Epoch: 2800, Training accuracy: 0.609375, Loss: 0.902089\n",
      "Epoch: 2900, Training accuracy: 0.671875, Loss: 0.928491\n",
      "Epoch: 3000, Training accuracy: 0.671875, Loss: 0.952961\n",
      "Epoch: 3100, Training accuracy: 0.648438, Loss: 0.974122\n",
      "Epoch: 3200, Training accuracy: 0.742188, Loss: 0.775832\n",
      "Epoch: 3300, Training accuracy: 0.640625, Loss: 0.967447\n",
      "Epoch: 3400, Training accuracy: 0.710938, Loss: 0.989242\n",
      "Epoch: 3500, Training accuracy: 0.656250, Loss: 0.950550\n",
      "Epoch: 3600, Training accuracy: 0.664062, Loss: 0.894659\n",
      "Epoch: 3700, Training accuracy: 0.750000, Loss: 0.658653\n",
      "Epoch: 3800, Training accuracy: 0.679688, Loss: 0.935508\n",
      "Epoch: 3900, Training accuracy: 0.640625, Loss: 1.238729\n",
      "Epoch: 4000, Training accuracy: 0.656250, Loss: 1.112322\n",
      "Epoch: 4100, Training accuracy: 0.710938, Loss: 0.889633\n",
      "Epoch: 4200, Training accuracy: 0.695312, Loss: 0.843709\n",
      "Epoch: 4300, Training accuracy: 0.601562, Loss: 0.955531\n",
      "Epoch: 4400, Training accuracy: 0.664062, Loss: 0.833880\n",
      "Epoch: 4500, Training accuracy: 0.640625, Loss: 0.983890\n",
      "Epoch: 4600, Training accuracy: 0.656250, Loss: 0.998966\n",
      "Epoch: 4700, Training accuracy: 0.625000, Loss: 1.012004\n",
      "Epoch: 4800, Training accuracy: 0.703125, Loss: 1.013445\n",
      "Epoch: 4900, Training accuracy: 0.601562, Loss: 1.168684\n",
      "Epoch: 5000, Training accuracy: 0.742188, Loss: 0.815980\n",
      "Epoch: 5100, Training accuracy: 0.695312, Loss: 0.885159\n",
      "Epoch: 5200, Training accuracy: 0.679688, Loss: 0.947390\n",
      "Epoch: 5300, Training accuracy: 0.757812, Loss: 0.762716\n",
      "Epoch: 5400, Training accuracy: 0.703125, Loss: 0.805878\n",
      "Epoch: 5500, Training accuracy: 0.609375, Loss: 1.032410\n",
      "Epoch: 5600, Training accuracy: 0.671875, Loss: 0.956119\n",
      "Epoch: 5700, Training accuracy: 0.695312, Loss: 0.947397\n",
      "Epoch: 5800, Training accuracy: 0.601562, Loss: 1.330493\n",
      "Epoch: 5900, Training accuracy: 0.742188, Loss: 0.779931\n",
      "Epoch: 6000, Training accuracy: 0.703125, Loss: 0.893038\n",
      "Epoch: 6100, Training accuracy: 0.664062, Loss: 0.919311\n",
      "Epoch: 6200, Training accuracy: 0.773438, Loss: 0.852026\n",
      "Epoch: 6300, Training accuracy: 0.750000, Loss: 0.758765\n",
      "Epoch: 6400, Training accuracy: 0.726562, Loss: 0.929841\n",
      "Epoch: 6500, Training accuracy: 0.625000, Loss: 1.109993\n",
      "Epoch: 6600, Training accuracy: 0.710938, Loss: 0.837849\n",
      "Epoch: 6700, Training accuracy: 0.718750, Loss: 1.052693\n",
      "Epoch: 6800, Training accuracy: 0.718750, Loss: 0.784819\n",
      "Epoch: 6900, Training accuracy: 0.718750, Loss: 0.745688\n",
      "Epoch: 7000, Training accuracy: 0.687500, Loss: 0.790240\n",
      "Epoch: 7100, Training accuracy: 0.695312, Loss: 1.001839\n",
      "Epoch: 7200, Training accuracy: 0.625000, Loss: 1.216611\n",
      "Epoch: 7300, Training accuracy: 0.718750, Loss: 0.856926\n",
      "Epoch: 7400, Training accuracy: 0.609375, Loss: 1.015961\n",
      "Epoch: 7500, Training accuracy: 0.640625, Loss: 0.882633\n",
      "Epoch: 7600, Training accuracy: 0.617188, Loss: 1.056147\n",
      "Epoch: 7700, Training accuracy: 0.687500, Loss: 0.907187\n",
      "Epoch: 7800, Training accuracy: 0.664062, Loss: 1.015623\n",
      "Epoch: 7900, Training accuracy: 0.648438, Loss: 0.990372\n",
      "Epoch: 8000, Training accuracy: 0.734375, Loss: 0.737405\n",
      "Epoch: 8100, Training accuracy: 0.664062, Loss: 1.196741\n",
      "Epoch: 8200, Training accuracy: 0.718750, Loss: 0.767392\n",
      "Epoch: 8300, Training accuracy: 0.562500, Loss: 1.129912\n",
      "Epoch: 8400, Training accuracy: 0.750000, Loss: 0.835938\n",
      "Epoch: 8500, Training accuracy: 0.695312, Loss: 0.962179\n",
      "Epoch: 8600, Training accuracy: 0.585938, Loss: 1.075287\n",
      "Epoch: 8700, Training accuracy: 0.656250, Loss: 0.889081\n",
      "Epoch: 8800, Training accuracy: 0.617188, Loss: 1.148062\n",
      "Epoch: 8900, Training accuracy: 0.601562, Loss: 1.105155\n",
      "Epoch: 9000, Training accuracy: 0.718750, Loss: 0.783011\n",
      "Epoch: 9100, Training accuracy: 0.656250, Loss: 1.113007\n",
      "Epoch: 9200, Training accuracy: 0.687500, Loss: 0.874371\n",
      "Epoch: 9300, Training accuracy: 0.703125, Loss: 0.789058\n",
      "Epoch: 9400, Training accuracy: 0.492188, Loss: 1.375320\n",
      "Epoch: 9500, Training accuracy: 0.640625, Loss: 0.873830\n",
      "Epoch: 9600, Training accuracy: 0.632812, Loss: 1.198574\n",
      "Epoch: 9700, Training accuracy: 0.757812, Loss: 0.801595\n",
      "Epoch: 9800, Training accuracy: 0.742188, Loss: 1.013231\n",
      "Epoch: 9900, Training accuracy: 0.710938, Loss: 0.873689\n",
      "Test accuracy: 0.609375\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets.cifar10 import load_data\n",
    "\n",
    "def next_batch(num, data, labels):\n",
    "    idx = np.arange(0, len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[i] for i in idx]\n",
    "    labels_shuffle = [labels[i] for i in idx]\n",
    "    \n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "\n",
    "def build_CNN_classifier(x):\n",
    "    x_image = x\n",
    "    \n",
    "    # 1st convolutional layer : one RGB image -> 64 features\n",
    "    W_conv1 = tf.Variable(tf.truncated_normal(shape = [5,5,3,64],stddev = 5e-2))  # shape= [height,width,input channel,out channel]\n",
    "    b_conv1 = tf.Variable(tf.constant(0.1,shape = [64]))\n",
    "    h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, strides = [1,1,1,1], padding = 'SAME') + b_conv1) # strides = [batch,height,width,channel]\n",
    "    \n",
    "    # 1st Pooling layer\n",
    "    h_pool1 = tf.nn.max_pool(h_conv1, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "    \n",
    "    # 2nd convolutional layer : 64 features -> 64 features\n",
    "    W_conv2 = tf.Variable(tf.truncated_normal(shape = [5,5,64,64], stddev = 5e-2))\n",
    "    b_conv2 = tf.Variable(tf.constant(0.1,shape = [64]))\n",
    "    h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, W_conv2, strides = [1,1,1,1], padding = 'SAME') + b_conv2)\n",
    "    \n",
    "    # 2nd pooling layer\n",
    "    h_pool2 = tf.nn.max_pool(h_conv2, ksize = [1,3,3,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "    \n",
    "    # 3rd convolutional layer\n",
    "    W_conv3 = tf.Variable(tf.truncated_normal(shape = [3,3,64,128], stddev = 5e-2))\n",
    "    b_conv3 = tf.Variable(tf.constant(0.1, shape = [128]))\n",
    "    h_conv3 = tf.nn.relu(tf.nn.conv2d(h_pool2, W_conv3, strides = [1,1,1,1], padding = 'SAME') + b_conv3)\n",
    "    \n",
    "    # 4th convolutional layer\n",
    "    W_conv4 = tf.Variable(tf.truncated_normal(shape = [3,3,128,128], stddev = 5e-2))\n",
    "    b_conv4 = tf.Variable(tf.constant(0.1, shape = [128]))\n",
    "    h_conv4 = tf.nn.relu(tf.nn.conv2d(h_conv3, W_conv4, strides = [1,1,1,1], padding = 'SAME') + b_conv4)\n",
    "    \n",
    "    # 5th convolution layer\n",
    "    W_conv5 = tf.Variable(tf.truncated_normal(shape = [3,3,128,128], stddev = 5e-2))\n",
    "    b_conv5 = tf.Variable(tf.constant(0.1, shape = [128]))\n",
    "    h_conv5 = tf.nn.relu(tf.nn.conv2d(h_conv4, W_conv5, strides = [1,1,1,1], padding = 'SAME') + b_conv5)\n",
    "    \n",
    "    \n",
    "    # Fully Connected Layer 1:2 times down sampling, 32*32 image -> 8*8*128 feature map\n",
    "    # 8*8*128 feature maps -> 384 features\n",
    "    W_fc1 = tf.Variable(tf.truncated_normal(shape = [8*8*128,384], stddev = 5e-2))\n",
    "    b_fc1 = tf.Variable(tf.constant(0.1, shape = [384]))\n",
    "    \n",
    "    h_conv5_flat = tf.reshape(h_conv5, [-1,8*8*128])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_conv5_flat, W_fc1) + b_fc1)\n",
    "    \n",
    "    # Drop out\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    \n",
    "    # Fully Connected Layer2 : 384 features -> one of 10 classes\n",
    "    W_fc2 = tf.Variable(tf.truncated_normal(shape = [384,10], stddev = 5e-2))\n",
    "    b_fc2 = tf.Variable(tf.constant(0.1, shape = [10]))\n",
    "    logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    y_pred = tf.nn.softmax(logits)\n",
    "    \n",
    "    return y_pred, logits\n",
    "\n",
    "\n",
    "# training and test\n",
    "x = tf.placeholder(tf.float32, shape = [None,32,32,3])\n",
    "y = tf.placeholder(tf.float32, shape = [None,10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "(x_train, y_train),(x_test,y_test) = load_data()\n",
    "# scalar label(0~9) -> One hot Encoding vector\n",
    "y_train_one_hot = tf.squeeze(tf.one_hot(y_train,10), axis = 1)\n",
    "y_test_one_hot = tf.squeeze(tf.one_hot(y_test,10), axis = 1)\n",
    "\n",
    "y_pred, logits = build_CNN_classifier(x)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = y, logits = logits))\n",
    "#train_step = tf.train.RMSPropOpimizer(1e-3).minimize(loss)\n",
    "train_step = tf.train.RMSPropOptimizer(1e-3).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(10000):\n",
    "        batch = next_batch(128, x_train, y_train_one_hot.eval())\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict = {x:batch[0],y:batch[1],keep_prob:1.0})\n",
    "            loss_print = loss.eval(feed_dict = {x:batch[0],y:batch[1],keep_prob:1.0})\\\n",
    "            \n",
    "            print(\"Epoch: %d, Training accuracy: %f, Loss: %f\" % (i,train_accuracy, loss_print))\n",
    "        # 20% drop out\n",
    "        sess.run(train_step, feed_dict = {x:batch[0],y:batch[1],keep_prob:0.8})\n",
    "    \n",
    "    test_accuracy = 0.0\n",
    "    for i in range(10):\n",
    "        test_batch = next_batch(1000,x_test,y_test_one_hot.eval())\n",
    "        test_accuracy = test_accuracy + accuracy.eval(feed_dict = {x:batch[0],y:batch[1],keep_prob:1.0})\n",
    "    test_accuracy = test_accuracy / 10;\n",
    "    print(\"Test accuracy: %f\" % test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
