{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dudco0040/dudco0040/blob/master/%EB%84%A4%EC%9D%B4%EB%B2%84_%EC%87%BC%ED%95%91_%EB%A6%AC%EB%B7%B0_%EA%B0%90%EC%84%B1_%EB%B6%84%EC%84%9D_%EB%AA%A8%EB%8D%B8_%EB%B9%84%EA%B5%90_%EC%A0%95%EB%A6%AC%EC%BD%94%EB%93%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ApQ5gTY3ynn"
      },
      "outputs": [],
      "source": [
        "# Colab에 Mecab 설치\n",
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab190912.sh\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from collections import Counter\n",
        "from konlpy.tag import Mecab\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/naver_shopping.txt\",\n",
        "filename=\"ratings_total.txt\")\n",
        "total_data = pd.read_table('ratings_total.txt', names=['ratings', 'reviews'])\n",
        "print('전체 리뷰 개수 :',len(total_data)) # 전체 리뷰 개수 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blIMTUzm32Jd"
      },
      "outputs": [],
      "source": [
        "#label(긍정,중립,부정) 분리하기\n",
        "#1:부정, 2~4:중립, 5:긍정\n",
        "total_data['label'] = np.select([(total_data.ratings==5),(total_data.ratings==4),(total_data.ratings==2),(total_data.ratings==1)], [1,0,0,-1])  # -1:부정, 0:중립, 1:긍정\n",
        "total_data[:5]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(total_data)"
      ],
      "metadata": {
        "id": "-NtIcIoQ6rAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdC5ad0V32Ls"
      },
      "outputs": [],
      "source": [
        "total_data['ratings'].nunique(), total_data['reviews'].nunique(), total_data['label'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_data.drop_duplicates(subset=['reviews'], inplace=True) # reviews 열에서 중복인 내용이 있다면 중복 제거\n",
        "print('총 샘플의 수 :',len(total_data))"
      ],
      "metadata": {
        "id": "qtb6OyJ158Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(total_data.isnull().values.any())"
      ],
      "metadata": {
        "id": "LVIPcg2S9vT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTpnQdS632N0"
      },
      "outputs": [],
      "source": [
        "#train/test data 나누기\n",
        "train_data, test_data = train_test_split(total_data, test_size = 0.25, random_state = 42)\n",
        "print('훈련용 리뷰의 개수 :', len(train_data))\n",
        "print('테스트용 리뷰의 개수 :', len(test_data))\n",
        "train_data['label'].value_counts().plot(kind = 'bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoFFoGO5-zH5"
      },
      "outputs": [],
      "source": [
        "train_data['ratings'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsFGKP1y-zZq"
      },
      "outputs": [],
      "source": [
        "train_data['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxkmb6-032QM"
      },
      "outputs": [],
      "source": [
        "# 한글과 공백을 제외하고 모두 제거\n",
        "train_data['reviews'] = train_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "train_data['reviews'].replace('', np.nan, inplace=True)\n",
        "print(train_data.isnull().sum())\n",
        "\n",
        "test_data.drop_duplicates(subset = ['reviews'], inplace=True) # 중복 제거\n",
        "test_data['reviews'] = test_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
        "test_data['reviews'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
        "test_data = test_data.dropna(how='any') # Null 값 제거\n",
        "print('전처리 후 테스트용 샘플의 개수 :',len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMZOnSXk32Sk"
      },
      "outputs": [],
      "source": [
        "mecab = Mecab()\n",
        "print(mecab.morphs('와 이런 것도 상품이라고 차라리 내가 만드는 게 나을 뻔'))\n",
        "\n",
        "#불용어 정의\n",
        "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게']\n",
        "\n",
        "train_data['tokenized'] = train_data['reviews'].apply(mecab.morphs)\n",
        "train_data['tokenized'] = train_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\n",
        "\n",
        "test_data['tokenized'] = test_data['reviews'].apply(mecab.morphs)\n",
        "test_data['tokenized'] = test_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\n",
        "\n",
        "#단어와 길이 분포 확인하기\n",
        "negative_words = np.hstack(train_data[train_data.label == -1]['tokenized'].values)\n",
        "neutrality_words = np.hstack(train_data[train_data.label == 0]['tokenized'].values)\n",
        "positive_words = np.hstack(train_data[train_data.label == 1]['tokenized'].values)\n",
        "\n",
        "negative_word_count = Counter(negative_words)\n",
        "neutrality_words_count = Counter(neutrality_words)\n",
        "positive_words_count = Counter(positive_words)\n",
        "\n",
        "#빈도수가 높은 상위 20개 단어만 출력\n",
        "print(\"부정 리뷰\")\n",
        "print(negative_word_count.most_common(20))\n",
        "print(\"중립 리뷰\")\n",
        "print(neutrality_words_count.most_common(20))\n",
        "print(\"긍정 리뷰\")\n",
        "print(positive_words_count.most_common(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGMgSIKC32U0"
      },
      "outputs": [],
      "source": [
        "fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(10,5))\n",
        "\n",
        "text_len = train_data[train_data['label']==1]['tokenized'].map(lambda x: len(x))\n",
        "ax1.hist(text_len, color='red')\n",
        "ax1.set_title('Positive Reviews')\n",
        "ax1.set_xlabel('length of samples')\n",
        "ax1.set_ylabel('number of samples')\n",
        "print('긍정 리뷰의 평균 길이 :', np.mean(text_len))\n",
        "\n",
        "text_len = train_data[train_data['label']==0]['tokenized'].map(lambda x: len(x))\n",
        "ax2.hist(text_len, color = 'green')\n",
        "ax2.set_title('Neutral Reviews')\n",
        "ax2.set_xlabel('length of samples')\n",
        "ax2.set_ylabel('number of samples')\n",
        "print('중립 리뷰의 평균 길이 :', np.mean(text_len))\n",
        "\n",
        "text_len = train_data[train_data['label']==-1]['tokenized'].map(lambda x: len(x))\n",
        "ax3.hist(text_len, color='blue')\n",
        "ax3.set_title('Negative Reviews')\n",
        "fig.suptitle('Words in texts')\n",
        "ax3.set_xlabel('length of samples')\n",
        "ax3.set_ylabel('number of samples')\n",
        "print('부정 리뷰의 평균 길이 :', np.mean(text_len))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vj9AP5aK32W1"
      },
      "outputs": [],
      "source": [
        "X_train = train_data['tokenized'].values\n",
        "y_train = train_data['label'].values\n",
        "\n",
        "X_test= test_data['tokenized'].values\n",
        "y_test = test_data['label'].values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#정수 인코딩\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)"
      ],
      "metadata": {
        "id": "G5DYnLQ6Pofc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfMhlN7-32eE"
      },
      "outputs": [],
      "source": [
        "threshold = 2\n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "  total_freq = total_freq + value\n",
        "  # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "  if(value < threshold):\n",
        "    rare_cnt = rare_cnt + 1\n",
        "    rare_freq = rare_freq + value\n",
        "    \n",
        "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwGTY-tr4fwc"
      },
      "outputs": [],
      "source": [
        "# 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거.\n",
        "# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\n",
        "vocab_size = total_cnt - rare_cnt + 2\n",
        "print('단어 집합의 크기 :',vocab_size)\n",
        "\n",
        "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "print(X_train[:3])\n",
        "print(X_test[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJY3nQ9FxkfT"
      },
      "outputs": [],
      "source": [
        "#one-hot encoding\n",
        "\n",
        "import numpy as np \n",
        "y_train_onehot = [] \n",
        "y_test_onehot = [] \n",
        "for i in range(len(y_train)): \n",
        "  if y_train[i] == 1: \n",
        "    y_train_onehot.append([0, 0, 1]) \n",
        "  elif y_train[i] == 0: \n",
        "    y_train_onehot.append([0, 1, 0]) \n",
        "  elif y_train[i] == -1: \n",
        "    y_train_onehot.append([1, 0, 0]) \n",
        "    \n",
        "  \n",
        "for i in range(len(y_test)): \n",
        "  if y_test[i] == 1:\n",
        "    y_test_onehot.append([0, 0, 1])\n",
        "  elif y_test[i] == 0: \n",
        "    y_test_onehot.append([0, 1, 0]) \n",
        "  elif y_test[i] == -1: \n",
        "    y_test_onehot.append([1, 0, 0]) \n",
        "\n",
        "y_train_onehot = np.array(y_train_onehot) \n",
        "y_test_onehot = np.array(y_test_onehot)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0azfywLDJu_"
      },
      "outputs": [],
      "source": [
        "y_train_onehot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPy99hoKDJ2G"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLr6n0ud4fzc"
      },
      "outputs": [],
      "source": [
        "#패딩\n",
        "print('리뷰의 최대 길이 :',max(len(l) for l in X_train))\n",
        "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
        "plt.hist([len(s) for s in X_train], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-yfedgq4f4b"
      },
      "outputs": [],
      "source": [
        "def below_threshold_len(max_len, nested_list):\n",
        "  count = 0\n",
        "  for sentence in nested_list:\n",
        "    if(len(sentence) <= max_len):\n",
        "      count = count + 1\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))\n",
        "  \n",
        "max_len = 80\n",
        "below_threshold_len(max_len, X_train)\n",
        "X_train = pad_sequences(X_train, maxlen = max_len)\n",
        "X_test = pad_sequences(X_test, maxlen = max_len)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train #패딩이 완료된 input data"
      ],
      "metadata": {
        "id": "4UZqq9fA772Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GRU**\n",
        "rmsprop epoch=15"
      ],
      "metadata": {
        "id": "kVD8ruL4pGZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, Dense, GRU\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(GRU(hidden_units))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "#verbose=1 로 지정하면, 언제 keras 에서 training 을 멈추었는지를 화면에 출력\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train_onehot, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "id": "1EYUir1Hk95g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n 테스트 정확도: {:.2f}%\".format(model.evaluate(X_test,y_test_onehot)[1]*100))"
      ],
      "metadata": {
        "id": "FUa0jzjvk9__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "epoch = 3으로 변경"
      ],
      "metadata": {
        "id": "hhEI5AlKERuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, Dense, GRU\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(GRU(hidden_units))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train_onehot, epochs=3, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "id": "ege4Wd_9tOH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n 테스트 정확도: {:.2f}%\".format(model.evaluate(X_test,y_test_onehot)[1]*100))"
      ],
      "metadata": {
        "id": "dsUFna8qtOKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GRU**\n",
        "adam epoch=15"
      ],
      "metadata": {
        "id": "-UgWTqmlpUQF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBpL1U2j4f7b"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Embedding, Dense, GRU\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(GRU(hidden_units))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train_onehot, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmM2eesT3YR0"
      },
      "outputs": [],
      "source": [
        "print(\"\\n 테스트 정확도: {:.2f}%\".format(model.evaluate(X_test,y_test_onehot)[1]*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "epoch =4 로 변경"
      ],
      "metadata": {
        "id": "JiGSr29epdvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, Dense, GRU\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(GRU(hidden_units))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train_onehot, epochs=2, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "id": "d56Bv-hHigk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n 테스트 정확도: {:.2f}%\".format(model.evaluate(X_test,y_test_onehot)[1]*100))"
      ],
      "metadata": {
        "id": "QRxp7K7cihpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GRU**\n",
        "rsmprop\n",
        "\n",
        "drop out\n"
      ],
      "metadata": {
        "id": "DtesqLC1VKVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, Dense, GRU\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import Dropout\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(GRU(hidden_units))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train_onehot, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "id": "hBXYMHkZVGlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n 테스트 정확도: {:.2f}%\".format(model.evaluate(X_test,y_test_onehot)[1]*100))"
      ],
      "metadata": {
        "id": "qY_n1PlsWTc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, Dense, GRU\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import Dropout\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(GRU(hidden_units))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train_onehot, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "id": "I4_ojIsXYuGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n 테스트 정확도: {:.2f}%\".format(model.evaluate(X_test,y_test_onehot)[1]*100))"
      ],
      "metadata": {
        "id": "hmkzH5aVYt8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM**\n",
        "adam"
      ],
      "metadata": {
        "id": "atDEU8IDphcU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qIBrdTa0W_Pf"
      },
      "outputs": [],
      "source": [
        "#LSTM 사용\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train_onehot, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29UrXOfddkgQ"
      },
      "outputs": [],
      "source": [
        "print(\"\\n 테스트 정확도: {:.2f}%\".format(model.evaluate(X_test,y_test_onehot)[1]*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM**\n",
        "rmsprop"
      ],
      "metadata": {
        "id": "Wf_ClNzDpqZk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WvnsyXWe-u2"
      },
      "outputs": [],
      "source": [
        "#LSTM 사용\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train_onehot, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIRpjkPGe-0I"
      },
      "outputs": [],
      "source": [
        "print(\"\\n 테스트 정확도: {:.2f}%\".format(model.evaluate(X_test,y_test_onehot)[1]*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "epoch = 4로 변경"
      ],
      "metadata": {
        "id": "29xEIawspzYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM 사용\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train_onehot, epochs=4, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "id": "T334CKLEkqlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n 테스트 정확도: {:.2f}%\".format(model.evaluate(X_test,y_test_onehot)[1]*100))"
      ],
      "metadata": {
        "id": "b_WnAupSkqo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM**\n",
        "rmsprop \n",
        "\n",
        "dropout 50%"
      ],
      "metadata": {
        "id": "vuvZQI86bCoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM 사용\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import Dropout\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train_onehot, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "id": "bUee43Wcar6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n 테스트 정확도: {:.2f}%\".format(model.evaluate(X_test,y_test_onehot)[1]*100))"
      ],
      "metadata": {
        "id": "7g--_kXnar-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "embedding_dim = 50\n",
        "hidden_units = 256\n"
      ],
      "metadata": {
        "id": "dfju4jnPOZVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM 사용\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "embedding_dim = 50\n",
        "hidden_units = 256\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train_onehot, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "id": "KQk095arp0XG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n 테스트 정확도: {:.2f}%\".format(model.evaluate(X_test,y_test_onehot)[1]*100))"
      ],
      "metadata": {
        "id": "KIA28-D-p4AY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**bi LSRM** \n",
        "rmsprop"
      ],
      "metadata": {
        "id": "BDWWy7PCwsY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(Bidirectional(LSTM(hidden_units)))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train_onehot, epochs=15, callbacks=[es, mc], batch_size=256, validation_split=0.2)"
      ],
      "metadata": {
        "id": "KUvMO_Bsv4Z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n 테스트 정확도: {:.2f}%\".format(model.evaluate(X_test,y_test_onehot)[1]*100))"
      ],
      "metadata": {
        "id": "QHWISGiOv4cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "embedding_dim = 100\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(LSTM(hidden_units))\n",
        "\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train_onehot, epochs=15, callbacks=[es, mc], batch_size=256, validation_split=0.2)"
      ],
      "metadata": {
        "id": "707UZilNv4eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ODSfyuoCv4gi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "네이버 쇼핑 리뷰 감성 분석_모델 비교 정리코드",
      "provenance": [],
      "authorship_tag": "ABX9TyMc8sNEoHfouJgsEGIAGNx7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}